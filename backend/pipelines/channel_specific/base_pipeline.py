"""
Base Pipeline for Self-Contained Channel Processing
Provides common functionality for all channel-specific pipelines.
"""

import os
import sys
import json
import yaml
import time
import logging
import tempfile
import shutil
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import torch
import numpy as np

logger = logging.getLogger(__name__)

class BasePipeline:
    """Base class for self-contained channel pipelines."""
    
    def __init__(self, channel_type: str, output_path: Optional[str] = None, base_model: str = "stable_diffusion_1_5"):
        self.channel_type = channel_type
        self.base_model = base_model
        self.models = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        self.vram_tier = self._detect_vram_tier()
        self.output_path = Path(output_path) if output_path else None
        
        self.supported_languages = {
            "en": {"name": "English", "bark_supported": True, "xtts_supported": True},
            "ja": {"name": "Japanese", "bark_supported": True, "xtts_supported": True},
            "es": {"name": "Spanish", "bark_supported": True, "xtts_supported": True},
            "zh": {"name": "Chinese", "bark_supported": False, "xtts_supported": True},
            "hi": {"name": "Hindi", "bark_supported": False, "xtts_supported": True},
            "ar": {"name": "Arabic", "bark_supported": False, "xtts_supported": True},
            "bn": {"name": "Bengali", "bark_supported": False, "xtts_supported": True},
            "pt": {"name": "Portuguese", "bark_supported": True, "xtts_supported": True},
            "ru": {"name": "Russian", "bark_supported": True, "xtts_supported": True},
            "fr": {"name": "French", "bark_supported": True, "xtts_supported": True},
            "de": {"name": "German", "bark_supported": True, "xtts_supported": True}
        }
    
    def _run_pipeline(self, input_path: str, output_path: str, base_model: str = "stable_diffusion_1_5", 
                     lora_models: Optional[List[str]] = None, lora_paths: Optional[Dict[str, str]] = None, 
                     db_run=None, db=None, render_fps: int = 24, output_fps: int = 24, 
                     frame_interpolation_enabled: bool = True, language: str = "en") -> str:
        """Core pipeline execution method."""
        try:
            logger.info(f"Starting {self.channel_type} pipeline")
            
            script_data = self.parse_input_script(input_path)
            
            expanded_script = self.expand_script_if_needed(script_data)
            
            output_dir = self.ensure_output_dir(output_path)
            
            scene_videos = []
            scenes = expanded_script.get('scenes', self._get_default_scenes())
            
            for i, scene in enumerate(scenes):
                scene_description = scene if isinstance(scene, str) else scene.get('description', f'Scene {i+1}')
                enhanced_prompt = self._enhance_prompt_for_channel(scene_description)
                
                scene_output = output_dir / f"scene_{i+1}.mp4"
                video_path = self.generate_video(enhanced_prompt, output_path=str(scene_output), duration=5.0)
                
                if video_path and os.path.exists(video_path):
                    scene_videos.append(video_path)
                    logger.info(f"Generated scene {i+1}: {video_path}")
            
            final_video = output_dir / "final_video.mp4"
            if scene_videos:
                self._combine_scene_videos(scene_videos, str(final_video))
            else:
                self._handle_video_generation_failure("Default content", 300.0, str(final_video))
            
            self.create_manifest(output_dir, 
                               scenes_generated=len(scene_videos),
                               final_video=str(final_video),
                               language=language)
            
            logger.info(f"{self.channel_type} pipeline completed: {final_video}")
            return str(output_dir)
            
        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}")
            output_dir = self.ensure_output_dir(output_path)
            error_log = output_dir / "pipeline_error.txt"
            with open(error_log, 'w') as f:
                f.write(f"Pipeline execution failed: {e}\n")
                f.write(f"Error occurred during pipeline processing\n")
            logger.error(f"Pipeline failed, error logged to: {error_log}")
            return str(output_dir)
    
    def _get_default_scenes(self) -> List[str]:
        """Get default scenes for the channel type."""
        return [
            f"Opening scene for {self.channel_type} content",
            f"Main action sequence in {self.channel_type} style",
            f"Character development scene for {self.channel_type}",
            f"Climactic moment in {self.channel_type} format",
            f"Closing scene for {self.channel_type} content"
        ]
    
    def _get_default_characters(self) -> List[Dict]:
        """Get default characters for the channel type."""
        return [
            {"name": "Character1", "description": f"Main character in {self.channel_type} style", "voice": "default_voice"},
            {"name": "Character2", "description": f"Supporting character in {self.channel_type} style", "voice": "default_voice"}
        ]
    
    def _enhance_prompt_for_channel(self, prompt: str) -> str:
        """Enhance prompt for specific channel style with detailed enhancements."""
        channel_enhancements = {
            "anime": {
                "prefix": "anime style, high quality animation, detailed character design",
                "style_tags": "cel shading, vibrant colors, expressive eyes, dynamic poses",
                "quality": "masterpiece, best quality, ultra detailed, 8k resolution"
            },
            "gaming": {
                "prefix": "gaming content, action-packed, dynamic gameplay",
                "style_tags": "realistic graphics, intense action, competitive gaming",
                "quality": "high resolution, smooth animation, professional gaming"
            },
            "superhero": {
                "prefix": "superhero style, heroic poses, dramatic lighting",
                "style_tags": "comic book style, powerful characters, epic scenes",
                "quality": "cinematic quality, dramatic composition, heroic atmosphere"
            },
            "manga": {
                "prefix": "manga style, black and white, detailed line art",
                "style_tags": "manga panels, expressive characters, detailed backgrounds",
                "quality": "high contrast, detailed illustration, professional manga"
            },
            "marvel_dc": {
                "prefix": "Marvel/DC comic style, superhero universe",
                "style_tags": "comic book art, iconic characters, action scenes",
                "quality": "comic book quality, detailed artwork, iconic style"
            },
            "original_manga": {
                "prefix": "original manga style, unique character design",
                "style_tags": "creative storytelling, original characters, manga aesthetics",
                "quality": "artistic quality, creative composition, original style"
            }
        }
        
        enhancement = channel_enhancements.get(self.channel_type, {
            "prefix": f"{self.channel_type} style",
            "style_tags": "high quality, detailed",
            "quality": "professional quality"
        })
        
        enhanced_prompt = f"{enhancement['prefix']}, {prompt}, {enhancement['style_tags']}, {enhancement['quality']}"
        return enhanced_prompt
    def _classify_scene_type(self, prompt: str) -> str:
        """Classify scene type for optimal model selection."""
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ["fight", "battle", "combat", "attack", "war"]):
            return "combat"
        elif any(word in prompt_lower for word in ["action", "chase", "run", "escape", "fast"]):
            return "action"
        elif any(word in prompt_lower for word in ["talk", "dialogue", "conversation", "speak", "discuss"]):
            return "dialogue"
        elif any(word in prompt_lower for word in ["explore", "discover", "journey", "travel", "walk"]):
            return "exploration"
        elif any(word in prompt_lower for word in ["emotion", "sad", "happy", "love", "memory", "feel"]):
            return "character_development"
        else:
            return "default"

        return f"{self.channel_type} style, {prompt}, high quality, detailed"
    
    def _combine_scene_videos(self, video_paths: List[str], output_path: str):
        """Combine multiple scene videos into final video using FFmpeg."""
        try:
            import subprocess
            import tempfile
            from pathlib import Path
            
            if not video_paths:
                logger.warning("No video paths provided for combination")
                return self._handle_video_generation_failure("Combined scenes", 300.0, output_path)
            
            valid_videos = [path for path in video_paths if os.path.exists(path)]
            if not valid_videos:
                logger.warning("No valid video files found for combination")
                return self._handle_video_generation_failure("Combined scenes", 300.0, output_path)
            
            logger.info(f"Combining {len(valid_videos)} scene videos using FFmpeg")
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                concat_file = f.name
                for video_path in valid_videos:
                    f.write(f"file '{os.path.abspath(video_path)}'\n")
            
            try:
                cmd = [
                    'ffmpeg', '-y',  # Overwrite output
                    '-f', 'concat',
                    '-safe', '0',
                    '-i', concat_file,
                    '-c', 'copy',  # Copy streams without re-encoding
                    '-avoid_negative_ts', 'make_zero',
                    output_path
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0 and os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    logger.info(f"Successfully combined {len(valid_videos)} scenes into {output_path} ({file_size} bytes)")
                    return output_path
                else:
                    logger.error(f"FFmpeg failed: {result.stderr}")
                    return self._combine_videos_opencv(valid_videos, output_path)
                    
            finally:
                if os.path.exists(concat_file):
                    os.unlink(concat_file)
                
        except Exception as e:
            logger.error(f"Error combining scene videos with FFmpeg: {e}")
            return self._combine_videos_opencv(valid_videos, output_path)
    
    def _combine_videos_opencv(self, video_paths: List[str], output_path: str) -> str:
        """Combine videos using MoviePy instead of OpenCV fallback."""
        try:
            from moviepy.editor import VideoFileClip, concatenate_videoclips
            
            logger.info(f"Combining {len(video_paths)} videos with MoviePy")
            
            if not video_paths:
                logger.error("No video paths provided for combination")
                return output_path
            
            clips = []
            for video_path in video_paths:
                try:
                    clip = VideoFileClip(video_path)
                    clips.append(clip)
                    logger.info(f"Loaded video clip: {video_path}")
                except Exception as e:
                    logger.warning(f"Could not load video {video_path}: {e}")
            
            if clips:
                final_video = concatenate_videoclips(clips)
                final_video.write_videofile(
                    output_path,
                    codec='libx264',
                    audio_codec='aac',
                    verbose=False,
                    logger=None
                )
                
                for clip in clips:
                    clip.close()
                final_video.close()
                
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    logger.info(f"MoviePy combined {len(video_paths)} scenes ({file_size} bytes)")
                    return output_path
                else:
                    logger.error("Failed to create combined video file")
                    return output_path
            else:
                logger.error("No valid video clips to combine")
                return output_path
                
        except Exception as e:
            logger.error(f"MoviePy video combination failed: {e}")
            return self._handle_video_generation_failure("Combined scenes", 300.0, output_path)
    
    def _initialize_language_support(self):
        """Initialize supported languages configuration."""
        self.supported_languages = {
            'en': {'name': 'English', 'voice_code': 'en', 'xtts_supported': True, 'bark_supported': True},
            'ja': {'name': 'Japanese', 'voice_code': 'ja', 'xtts_supported': True, 'bark_supported': True},
            'es': {'name': 'Spanish', 'voice_code': 'es', 'xtts_supported': True, 'bark_supported': True},
            'zh': {'name': 'Chinese', 'voice_code': 'zh-cn', 'xtts_supported': True, 'bark_supported': False},
            'hi': {'name': 'Hindi', 'voice_code': 'hi', 'xtts_supported': True, 'bark_supported': False},
            'ar': {'name': 'Arabic', 'voice_code': 'ar', 'xtts_supported': True, 'bark_supported': False},
            'bn': {'name': 'Bengali', 'voice_code': 'bn', 'xtts_supported': True, 'bark_supported': False},
            'pt': {'name': 'Portuguese', 'voice_code': 'pt', 'xtts_supported': True, 'bark_supported': True},
            'ru': {'name': 'Russian', 'voice_code': 'ru', 'xtts_supported': True, 'bark_supported': False},
            'fr': {'name': 'French', 'voice_code': 'fr', 'xtts_supported': True, 'bark_supported': True},
            'de': {'name': 'German', 'voice_code': 'de', 'xtts_supported': True, 'bark_supported': True}
        }
    
    def _detect_vram_tier(self) -> str:
        """Detect VRAM tier for model selection."""
        if not torch.cuda.is_available():
            return "cpu"
        
        try:
            vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if vram_gb >= 24:
                return "extreme"
            elif vram_gb >= 16:
                return "high"
            elif vram_gb >= 8:
                return "medium"
            else:
                return "low"
        except:
            return "medium"
    
    def load_llm_model(self, model_name: str = "deepseek_llama_8b_peft"):
        """Load LLM model with Deepseek support."""
        if "llm" in self.models:
            return self.models["llm"]
        
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            try:
                from ..model_manager import TEXT_MODELS, HF_MODEL_REPOS
                model_info = TEXT_MODELS.get(model_name)
                if not model_info:
                    logger.warning(f"Model {model_name} not found, using fallback")
                    model_name = "dialogpt_medium"
                    model_info = TEXT_MODELS.get(model_name)
                
                model_id = model_info.get("model_id", "microsoft/DialoGPT-medium")
            except ImportError:
                logger.warning("Model manager not available, using default models")
                model_mapping = {
                    "deepseek_llama_8b_peft": "deepseek-ai/deepseek-llm-7b-chat",
                    "dialogpt_medium": "microsoft/DialoGPT-medium",
                    "gpt2": "gpt2",
                    "distilgpt2": "distilgpt2"
                }
                model_id = model_mapping.get(model_name, "microsoft/DialoGPT-medium")
            
            logger.info(f"Loading LLM model: {model_id}")
            
            tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            vram_tier = self._detect_vram_tier()
            
            if vram_tier in ["high", "ultra"] and torch.cuda.is_available():
                model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
            elif vram_tier == "medium" and torch.cuda.is_available():
                model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16,
                    device_map="cuda",
                    trust_remote_code=True
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True
                )
            
            def generate_text(prompt: str, max_tokens: int = 100) -> str:
                try:
                    inputs = tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=512)
                    if torch.cuda.is_available() and hasattr(model, 'device') and model.device.type == "cuda":
                        inputs = inputs.to("cuda")
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            inputs,
                            max_new_tokens=max_tokens,
                            do_sample=True,
                            temperature=0.7,
                            top_p=0.9,
                            pad_token_id=tokenizer.eos_token_id
                        )
                    
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    if generated_text.startswith(prompt):
                        generated_text = generated_text[len(prompt):].strip()
                    
                    return generated_text
                    
                except Exception as e:
                    logger.error(f"Error in text generation: {e}")
                    return f"Enhanced {prompt} with detailed character interactions and dynamic scenes"
            
            self.models["llm"] = {
                "model": model,
                "tokenizer": tokenizer,
                "generate": generate_text,
                "device": model.device.type if hasattr(model, 'device') else "cpu"
            }
            
            logger.info(f"LLM model {model_id} loaded successfully")
            return self.models["llm"]
            
        except Exception as e:
            logger.error(f"Failed to load LLM model {model_name}: {e}")
            
            logger.error("All LLM models failed to load, no fallback available")
            self.models["llm"] = None
            return self.models["llm"]
    
    def _handle_music_generation_failure(self, prompt: str, duration: float, output_path: str) -> bool:
        """Handle music generation failure by trying alternative models."""
        try:
            from ..ai_music_generator import AIMusicGenerator
            music_generator = AIMusicGenerator(vram_tier=self.vram_tier)
            
            alternative_models = ["musicgen_small", "musicgen_medium"]
            for model_name in alternative_models:
                if music_generator.load_model(model_name):
                    return music_generator.generate_music(prompt, model_name, output_path, duration)
            
            logger.error("All music models failed, cannot generate music")
            return False
            
        except Exception as e:
            logger.error(f"Error in music generation failure handling: {e}")
            return False
    def _handle_llm_generation_failure(self, prompt: str, max_tokens: int = 100) -> str:
        """Handle LLM generation failure by trying alternative models."""
        try:
            alternative_models = ["deepseek", "llama", "mistral"]
            for model_name in alternative_models:
                try:
                    llm_model = self.load_llm_model(model_name)
                    if llm_model and llm_model.get("generate"):
                        result = llm_model["generate"](prompt, max_tokens=max_tokens)
                        if result and len(result.strip()) > 10:
                            logger.info(f"Successfully generated text with alternative LLM: {model_name}")
                            return result
                except Exception as e:
                    logger.warning(f"Alternative LLM model {model_name} failed: {e}")
                    continue
            
            logger.error("All LLM models failed, cannot generate text")
            return prompt
            
        except Exception as e:
            logger.error(f"Error in LLM generation failure handling: {e}")
            return prompt
    

            return ""
    
    def _generate_with_llm(self, model, tokenizer, prompt: str, max_tokens: int = 100) -> str:
        """Generate text using loaded LLM model with thread-safe timeout protection."""
        try:
            import torch
            import threading
            import queue
            
            result_queue = queue.Queue()
            timeout_occurred = threading.Event()
            
            def llm_worker():
                try:
                    # Format prompt for better content generation
                    if "title" in prompt.lower():
                        formatted_prompt = f"YouTube video title about {prompt.replace('Generate a YouTube title for', '').replace('Create an engaging title for', '').strip()}: "
                    elif "description" in prompt.lower():
                        formatted_prompt = f"YouTube video description about {prompt.replace('Generate a YouTube description for', '').strip()}: "
                    elif "next episode" in prompt.lower():
                        formatted_prompt = f"Next episode suggestions for {prompt.replace('Generate next episode suggestions for', '').strip()}: 1."
                    else:
                        formatted_prompt = f"{prompt}: "
                    
                    if tokenizer.pad_token is None:
                        tokenizer.pad_token = tokenizer.eos_token
                    
                    inputs = tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=256)
                    if torch.cuda.is_available() and self.vram_tier != "low":
                        inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=min(max_tokens, 50),  # Limit tokens for faster generation
                            do_sample=True,
                            temperature=0.8,
                            top_p=0.9,
                            repetition_penalty=1.1,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            num_return_sequences=1,
                            num_beams=1,  # Faster generation
                            early_stopping=True
                        )
                    
                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    
                    if formatted_prompt in generated_text:
                        response = generated_text.replace(formatted_prompt, "").strip()
                    else:
                        response = generated_text.strip()
                    
                    if response:
                        response = response.replace('\n', ' ').strip()
                        
                        if "title" in prompt.lower():
                            response = response.replace('YouTube video title about', '').replace('Title:', '').strip()
                            response = response.replace('Create a YouTube Title for', '').replace('Description (optional)', '').strip()
                            
                            if '.' in response:
                                response = response.split('.')[0].strip()
                            if response.endswith(('!', '?')):
                                pass  # Keep exclamation/question marks
                            elif not response.endswith(('!', '?', ':')):
                                response = response.strip() + '!'
                        
                        if len(response) < 5 or response.lower() in ['title', 'description', 'content', 'your name, your profile picture and more']:
                            result_queue.put(self._generate_fallback_content_for_prompt(prompt))
                        else:
                            result_queue.put(response[:200])  # Reasonable max length
                    else:
                        result_queue.put(self._generate_fallback_content_for_prompt(prompt))
                        
                except Exception as e:
                    result_queue.put(self._generate_fallback_content_for_prompt(prompt))
            
            worker_thread = threading.Thread(target=llm_worker)
            worker_thread.daemon = True
            worker_thread.start()
            
            try:
                result = result_queue.get(timeout=30)
                return result
            except queue.Empty:
                timeout_occurred.set()
                logger.warning("LLM generation timed out, using fallback")
                return self._generate_fallback_content_for_prompt(prompt)
            
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            return self._generate_fallback_content_for_prompt(prompt)
    
    def _handle_llm_content_failure(self, prompt: str) -> str:
        """Handle LLM content failure by attempting alternative models."""
        try:
            alternative_models = ["deepseek_llama_8b_peft", "llama_7b", "mistral_7b"]
            for model_name in alternative_models:
                if self.load_llm_model(model_name):
                    return self._generate_with_llm(prompt, max_tokens=100)
            
            logger.error("All LLM models failed, cannot generate content")
            return f"LLM generation failed for: {prompt[:50]}..."
            
        except Exception as e:
            logger.error(f"Error in LLM content failure handling: {e}")
            return f"LLM processing error for: {prompt[:50]}..."
    
    def _load_video_model(self, model_name: Optional[str] = None):
        """Load video generation model."""
        if model_name is None:
            model_name = self.base_model
        if model_name in self.models:
            return self.models[model_name]
        
        try:
            class VideoModel:
                def __init__(self, model_name, device):
                    self.model_name = model_name
                    self.device = device
                    self.vram_tier = "medium"
                    self._load_model()
                
                def _load_model(self):
                    try:
                        if "animatediff" in self.model_name.lower():
                            from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler
                            import torch
                            
                            adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")
                            self.pipe = AnimateDiffPipeline.from_pretrained(
                                "runwayml/stable-diffusion-v1-5", 
                                motion_adapter=adapter,
                                torch_dtype=torch.float16 if self.vram_tier != "low" else torch.float32
                            )
                            
                            if self.device == "cuda":
                                self.pipe = self.pipe.to("cuda")
                            
                            self.pipe.scheduler = EulerDiscreteScheduler.from_config(self.pipe.scheduler.config)
                            self.loaded = True
                            
                        elif "svd" in self.model_name.lower():
                            from diffusers import StableVideoDiffusionPipeline
                            import torch
                            
                            self.pipe = StableVideoDiffusionPipeline.from_pretrained(
                                "stabilityai/stable-video-diffusion-img2vid-xt",
                                torch_dtype=torch.float16 if self.vram_tier != "low" else torch.float32
                            )
                            
                            if self.device == "cuda":
                                self.pipe = self.pipe.to("cuda")
                            
                            self.loaded = True
                            
                        else:
                            self.loaded = False
                            
                    except Exception as e:
                        logger.error(f"Failed to load video model {self.model_name}: {e}")
                        self.loaded = False
                
                def generate_video(self, prompt, duration=10.0, output_path=None, **kwargs):
                    """Generate video from prompt."""
                    try:
                        import cv2
                        import numpy as np
                        
                        if not output_path:
                            output_path = f"temp_video_{hash(prompt)}.mp4"
                        
                        os.makedirs(os.path.dirname(output_path), exist_ok=True)
                        
                        if hasattr(self, 'loaded') and self.loaded and hasattr(self, 'pipe'):
                            try:
                                if "animatediff" in self.model_name.lower():
                                    import torch
                                    video_frames = self.pipe(
                                        prompt,
                                        num_frames=min(16, int(duration * 24 / 2)),
                                        guidance_scale=7.5,
                                        num_inference_steps=25,
                                        generator=torch.Generator("cpu").manual_seed(42)
                                    ).frames[0]
                                    
                                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                                    out = cv2.VideoWriter(output_path, fourcc, 24, (1920, 1080))
                                    
                                    for frame in video_frames:
                                        frame_np = np.array(frame)
                                        frame_resized = cv2.resize(frame_np, (1920, 1080))
                                        frame_bgr = cv2.cvtColor(frame_resized, cv2.COLOR_RGB2BGR)
                                        out.write(frame_bgr)
                                    
                                    out.release()
                                    return True
                                    
                            except Exception as e:
                                logger.error(f"AI video generation failed: {e}, using fallback")
                        
                        duration = min(duration, 5.0)
                        fps = 24
                        frames = int(duration * fps)
                        width, height = 1920, 1080
                        
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
                        
                        if not out.isOpened():
                            return False
                        
                        for i in range(min(frames, 120)):
                            frame = np.zeros((height, width, 3), dtype=np.uint8)
                            frame[:] = (50, 50, 100)
                            
                            try:
                                cv2.putText(frame, "Generated Video", 
                                           (50, height//2), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)
                                cv2.putText(frame, f"Frame {i+1}", 
                                           (50, height//2 + 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (200, 200, 200), 2)
                            except Exception as e:
                                logger.error(f"Error adding text overlay: {e}")
                            
                            out.write(frame)
                        
                        out.release()
                        return True
                    except Exception:
                        return False
            
            video_model = VideoModel(model_name, self.device)
            self.models[model_name] = video_model
            logger.info(f"Video model loaded: {model_name}")
            return video_model
                
        except Exception as e:
            logger.error(f"Failed to load video model {model_name}: {e}")
            return None
    
    def load_voice_model(self, model_type: str = "bark"):
        """Load voice generation model."""
        model_key = f"voice_{model_type}"
        if model_key in self.models:
            return self.models[model_key]
        
        try:
            if model_type == "bark":
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        
                    from bark import SAMPLE_RATE, generate_audio, preload_models
                    
                    if self.vram_tier in ["low", "medium"]:
                        os.environ["CUDA_VISIBLE_DEVICES"] = ""
                        
                    preload_models()
                    
                    def bark_generate_wrapper(text, voice_preset="v2/en_speaker_6"):
                        try:
                            audio_array = generate_audio(text, history_prompt=voice_preset)
                            return audio_array, SAMPLE_RATE
                        except Exception as e:
                            logger.error(f"Bark generation failed: {e}")
                            return None
                    
                    self.models[model_key] = {
                        "type": "bark",
                        "loaded": True,
                        "generate": bark_generate_wrapper
                    }
                    logger.info("Bark model loaded successfully")
                    
                except Exception as e:
                    logger.warning(f"Bark loading failed: {e}. Using fallback audio generation.")
                    logger.error("Failed to load Bark model, no fallback available")
                    return None
            elif model_type == "xtts":
                retry_count = 0
                max_retries = 3
                while retry_count < max_retries:
                    try:
                        from TTS.api import TTS
                        logger.info(f"Loading XTTS-v2 model with {self.vram_tier} VRAM optimizations (attempt {retry_count + 1})")
                        gpu_enabled = self.vram_tier != "low"
                        model = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=gpu_enabled)
                        self.models[model_key] = {
                            "type": "xtts", 
                            "model": model, 
                            "languages": ["en", "es", "fr", "de", "it", "pt", "pl", "tr", "ru", "nl", "cs", "ar", "zh-cn", "ja", "hu", "ko"]
                        }
                        logger.info(f"Successfully loaded XTTS model on attempt {retry_count + 1}")
                        break
                    except ImportError as e:
                        logger.error(f"XTTS import failed on attempt {retry_count + 1}: {e}")
                        retry_count += 1
                        if retry_count >= max_retries:
                            logger.error("Failed to load XTTS after all retries, using fallback")
                            return None
                    except Exception as e:
                        logger.error(f"XTTS loading failed on attempt {retry_count + 1}: {e}")
                        retry_count += 1
                        if retry_count >= max_retries:
                            logger.error("Failed to load XTTS after all retries, using fallback")
                            return None
            else:
                logger.warning(f"Unknown voice model: {model_type}")
                return None
                
            logger.info(f"Voice model {model_type} loaded successfully")
            return self.models[model_key]
                
        except Exception as e:
            logger.error(f"Critical failure loading voice model {model_type}: {e}")
            logger.info("Attempting emergency fallback voice generation")
            logger.error("Voice model failed to load, no fallback available")
            return None
            return self.models[model_key]
    
    def load_music_model(self, model_type: str = "musicgen"):
        """Load music generation model."""
        model_key = f"music_{model_type}"
        if model_key in self.models:
            return self.models[model_key]
        
        try:
            if model_type == "musicgen":
                retry_count = 0
                max_retries = 3
                while retry_count < max_retries:
                    try:
                        from audiocraft.models import MusicGen
                        logger.info(f"Loading MusicGen model with {self.vram_tier} VRAM optimizations (attempt {retry_count + 1})")
                        model_size = "small" if self.vram_tier in ["low", "medium"] else "medium"
                        model = MusicGen.get_pretrained(f'facebook/musicgen-{model_size}')
                        self.models[model_key] = {
                            "type": "musicgen",
                            "model": model,
                            "generate": lambda prompt, duration: model.generate([prompt], duration=duration)
                        }
                        logger.info(f"Successfully loaded MusicGen model on attempt {retry_count + 1}")
                        break
                    except ImportError as e:
                        logger.warning(f"audiocraft not available: {e}. Using fallback music generation.")
                        self.models[model_key] = {
                            "type": "fallback", 
                            "loaded": True,
                            "generate": lambda **kwargs: self._generate_fallback_music(
                                descriptions=kwargs.get('descriptions', []),
                                duration=kwargs.get('duration', 30.0),
                                prompt=kwargs.get('prompt', None)
                            )
                        }
                        break
                    except Exception as e:
                        logger.error(f"MusicGen loading failed on attempt {retry_count + 1}: {e}")
                        retry_count += 1
                        if retry_count >= max_retries:
                            logger.error("Failed to load MusicGen after all retries, using fallback")
                            logger.error("MusicGen failed to load after all retries, no fallback available")
                            return None
            else:
                logger.warning(f"Unknown music model: {model_type}")
                return None
                
            logger.info(f"Music model {model_type} loaded successfully")
            return self.models[model_key]
                
        except Exception as e:
            logger.error(f"Critical failure loading music model {model_type}: {e}")
            logger.info("Attempting emergency fallback music generation")
            logger.error("Critical failure loading music model, no fallback available")
            return None
            return self.models[model_key]
    def _musicgen_generate_wrapper(self, model, **kwargs):
        """Wrapper for MusicGen generation that handles tensor conversion."""
        try:
            prompt = kwargs.get("prompt", "")
            duration = kwargs.get("duration", 30.0)
            output_path = kwargs.get("output_path", "/tmp/music.wav")
            
            model.set_generation_params(duration=duration)
            wav = model.generate([prompt])
            
            if hasattr(wav[0], 'cpu'):
                audio_array = wav[0].cpu().numpy()
            else:
                audio_array = wav[0]
            
            import scipy.io.wavfile as wavfile
            import numpy as np
            sample_rate = getattr(model, 'sample_rate', 32000)
            audio_array = audio_array.squeeze()
            if audio_array.ndim > 1:
                audio_array = audio_array[0]
            audio_array = (audio_array * 32767).astype(np.int16)
            wavfile.write(output_path, sample_rate, audio_array)
            
            return os.path.exists(output_path)
        except Exception as e:
            logger.error(f"MusicGen wrapper error: {e}")
            return False


    
    def expand_script_if_needed(self, script_data: Dict, min_duration: float = 20.0) -> Dict:
        """Expand script to target duration using LLM."""
        current_duration = sum(scene.get('duration', 5.0) for scene in script_data.get('scenes', []))
        
        if current_duration >= min_duration * 60:
            return script_data
        
        llm = self.load_llm_model()
        if not llm:
            return self._expand_script_fallback(script_data, min_duration)
        
        try:
            scenes = script_data.get('scenes', [])
            expanded_scenes = []
            
            for scene in scenes:
                expanded_scene = scene.copy()
                
                if len(scene.get('description', '')) < 100:
                    prompt = f"Expand this {self.channel_type} scene with more detail: {scene.get('description', '')}"
                    
                    expanded_text = llm["generate"](prompt, max_tokens=100)
                    if expanded_text and len(expanded_text.strip()) > 0:
                        scene['description'] = expanded_text.strip()
                        expanded_scene['duration'] = max(scene.get('duration', 5.0) * 1.5, 8.0)
                    else:
                        scene['description'] = f"Enhanced {scene.get('description', 'scene')} with detailed character interactions and dynamic action sequences"
                
                expanded_scenes.append(expanded_scene)
            
            script_data['scenes'] = expanded_scenes
            logger.info(f"Script expanded from {current_duration:.1f}s to target {min_duration*60}s")
            return script_data
            
        except Exception as e:
            logger.error(f"LLM script expansion failed: {e}")
            return self._expand_script_fallback(script_data, min_duration)
        finally:
            if 'llm' in self.models and self.models['llm']:
                try:
                    torch.cuda.empty_cache()
                except Exception as e:
                    logger.error(f"Error during model cleanup: {e}")
    
    def _expand_script_fallback(self, script_data: Dict, min_duration: float) -> Dict:
        """Fallback script expansion without LLM."""
        scenes = script_data.get('scenes', [])
        if not scenes:
            scenes = self._get_default_scenes()
        
        target_duration = min_duration * 60
        current_duration = sum(scene.get('duration', 5.0) if isinstance(scene, dict) else 5.0 for scene in scenes)
        
        expanded_scenes = []
        for i, scene in enumerate(scenes):
            if isinstance(scene, dict):
                expanded_scene = scene.copy()
                original_desc = scene.get('description', f'Scene {i+1}')
                expanded_scene['description'] = f"Enhanced {original_desc} with detailed character development, dynamic action sequences, and immersive {self.channel_type} atmosphere"
                
                if current_duration < target_duration:
                    multiplier = target_duration / max(current_duration, 1.0)
                    expanded_scene['duration'] = scene.get('duration', 5.0) * multiplier
                else:
                    expanded_scene['duration'] = scene.get('duration', 5.0)
            else:
                expanded_scene = {
                    'description': f"Enhanced Scene {i+1}: {scene} with detailed character development, dynamic action sequences, and immersive {self.channel_type} atmosphere",
                    'duration': 5.0 * (target_duration / max(current_duration, 1.0) if current_duration < target_duration else 1.0)
                }
            
            expanded_scenes.append(expanded_scene)
        
        script_data['scenes'] = expanded_scenes
        script_data['expanded'] = True
        logger.info(f"Fallback script expansion completed for {len(expanded_scenes)} scenes")
        return script_data
    
    def generate_combat_scene(self, scene_description: str, duration: float, characters: List[Dict], 
                            style: Optional[str] = None, difficulty: str = "medium") -> Dict:
        """Generate combat scene data."""
        if style is None:
            style = self.channel_type
        
        combat_types = {
            "melee": ["punch", "kick", "block", "dodge"],
            "ranged": ["aim", "shoot", "reload", "cover"],
            "magic": ["cast", "channel", "summon", "shield"],
            "aerial": ["fly", "dive", "hover", "dash"]
        }
        
        combat_type = "melee"
        if any(word in scene_description.lower() for word in ["gun", "shoot", "bullet"]):
            combat_type = "ranged"
        elif any(word in scene_description.lower() for word in ["magic", "spell", "energy"]):
            combat_type = "magic"
        elif any(word in scene_description.lower() for word in ["fly", "aerial", "sky"]):
            combat_type = "aerial"
        
        moves_per_second = {"easy": 0.5, "medium": 1.0, "hard": 1.5, "epic": 2.0}
        total_moves = int(duration * moves_per_second.get(difficulty, 1.0))
        
        sequences = []
        for i in range(total_moves):
            sequence = {
                "sequence_id": i + 1,
                "start_time": i * (duration / max(total_moves, 1)),
                "duration": duration / max(total_moves, 1),
                "movement": combat_types[combat_type][i % len(combat_types[combat_type])],
                "intensity": min(0.5 + (i / max(total_moves - 1, 1)) * 0.5, 1.0)
            }
            sequences.append(sequence)
        
        return {
            "scene_type": "combat",
            "combat_type": combat_type,
            "duration": duration,
            "style": style,
            "difficulty": difficulty,
            "sequences": sequences,
            "video_prompt": f"{style} style {combat_type} combat scene, {difficulty} difficulty, {duration}s duration, high quality animation"
        }
    
    def _process_script_with_llm(self, script_data: Dict, channel_type: str = "anime") -> Dict:
        """Process script with LLM to create enhanced model-specific prompts."""
        try:
            llm_model = self.load_llm_model()
            if not llm_model or not llm_model.get("generate"):
                logger.warning("LLM not available, using fallback processing")
                return self._fallback_script_processing(script_data, channel_type)
            
            enhanced_scenes = []
            scenes = script_data.get('scenes', [])
            characters = script_data.get('characters', [])
            locations = script_data.get('locations', [])
            
            for i, scene in enumerate(scenes):
                if isinstance(scene, dict):
                    scene_text = scene.get('description', f'Scene {i+1}')
                    character = scene.get('character', '')
                    dialogue = scene.get('dialogue', '')
                    location = scene.get('location', '')
                    
                    if character and dialogue:
                        scene_text = f"{scene_text}. Character {character} says: '{dialogue}'"
                    if location:
                        scene_text = f"{scene_text}. Location: {location}"
                else:
                    scene_text = scene if isinstance(scene, str) else f'Scene {i+1}'
                
                analysis_prompt = f"""
Analyze this {channel_type} scene and create detailed prompts for AI generation models.

Scene: {scene_text}
Characters: {[c.get('name', str(c)) if isinstance(c, dict) else str(c) for c in characters]}
Locations: {[l.get('name', str(l)) if isinstance(l, dict) else str(l) for l in locations]}

Create a JSON response with these fields:
{{
    "video_prompt": "Detailed prompt for video generation with visual elements, camera angles, lighting, style",
    "voice_prompt": "Dialogue or narration text with emotional tone and character voice instructions", 
    "music_prompt": "Background music description with mood, instruments, tempo, style",
    "scene_type": "action/dialogue/exploration/combat/character_development",
    "visual_elements": ["list", "of", "key", "visual", "components"],
    "audio_elements": ["list", "of", "key", "audio", "components"],
    "duration": estimated_duration_in_seconds,
    "quality_keywords": ["keywords", "for", "maximum", "quality"]
}}

Focus on creating prompts that will generate the highest quality {channel_type} content. Be specific about visual details, character expressions, environmental elements, and audio characteristics.
"""
                
                try:
                    llm_response = llm_model["generate"](analysis_prompt, max_tokens=500)
                    if not llm_response or len(llm_response.strip()) < 10:
                        llm_response = self._handle_llm_generation_failure(analysis_prompt, 500)
                    
                    import json
                    import re
                    json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
                    if json_match:
                        scene_analysis = json.loads(json_match.group())
                        
                        enhanced_scene = self._enhance_scene_for_channel(scene_analysis, channel_type)
                        enhanced_scene['original_description'] = scene_text
                        enhanced_scene['scene_number'] = i + 1
                        enhanced_scenes.append(enhanced_scene)
                    else:
                        enhanced_scenes.append(self._handle_llm_scene_failure(scene_text, i + 1, channel_type))
                        
                except Exception as e:
                    logger.error(f"LLM scene analysis failed for scene {i+1}: {e}")
                    enhanced_scenes.append(self._handle_llm_scene_failure(scene_text, i + 1, channel_type))
            
            processed_script = script_data.copy()
            processed_script['enhanced_scenes'] = enhanced_scenes
            processed_script['llm_processed'] = True
            
            logger.info(f"Successfully processed {len(enhanced_scenes)} scenes with LLM")
            return processed_script
            
        except Exception as e:
            logger.error(f"Script processing with LLM failed: {e}")
            return self._fallback_script_processing(script_data, channel_type)
    
    def _enhance_scene_for_channel(self, scene_analysis: Dict, channel_type: str) -> Dict:
        """Add channel-specific enhancements to LLM scene analysis."""
        channel_enhancements = {
            "anime": {
                "video_prefix": "masterpiece, best quality, ultra detailed, 8k resolution, cinematic lighting, smooth animation, professional anime style, vibrant colors, dynamic composition, ",
                "video_suffix": ", 16:9 aspect ratio, smooth motion, professional cinematography, ultra high definition",
                "music_style": "anime soundtrack, orchestral, emotional, "
            },
            "gaming": {
                "video_prefix": "gaming footage, high action, dynamic camera, intense gameplay, ",
                "video_suffix": ", gaming aesthetics, competitive scene",
                "music_style": "gaming music, electronic, energetic, "
            }
        }
        
        enhancements = channel_enhancements.get(channel_type, channel_enhancements["anime"])
        
        if 'video_prompt' in scene_analysis:
            scene_analysis['video_prompt'] = f"{enhancements['video_prefix']}{scene_analysis['video_prompt']}{enhancements['video_suffix']}"
        
        if 'music_prompt' in scene_analysis:
            scene_analysis['music_prompt'] = f"{enhancements['music_style']}{scene_analysis['music_prompt']}"
        
        return scene_analysis
    
    def _handle_llm_scene_failure(self, scene_text: str, scene_number: int, channel_type: str) -> Dict:
        """Handle LLM scene processing failure by attempting alternative approaches."""
        try:
            logger.warning(f"LLM scene processing failed for scene {scene_number}, using basic analysis")
            
            # Extract basic information from scene text
            duration = 10.0
            if "long" in scene_text.lower() or "extended" in scene_text.lower():
                duration = 15.0
            elif "short" in scene_text.lower() or "brief" in scene_text.lower():
                duration = 8.0
            
            scene_type = "dialogue"
            if any(word in scene_text.lower() for word in ["fight", "battle", "combat", "action"]):
                scene_type = "action"
            elif any(word in scene_text.lower() for word in ["peaceful", "calm", "quiet", "serene"]):
                scene_type = "ambient"
            
            return {
                "scene_number": scene_number,
                "original_description": scene_text,
                "video_prompt": f"high quality {channel_type} {scene_type} scene, {scene_text}, detailed animation, professional quality",
                "voice_prompt": scene_text,
                "music_prompt": f"{channel_type} {scene_type} background music, atmospheric",
                "scene_type": scene_type,
                "visual_elements": ["characters", "environment", "lighting"],
                "audio_elements": ["dialogue", "background_music", "ambient_sounds"],
                "duration": duration,
                "quality_keywords": ["high_quality", "detailed", "professional", scene_type],
                "llm_processed": False
            }
            
        except Exception as e:
            logger.error(f"Error in LLM scene failure handling: {e}")
            return None
    
    def _fallback_script_processing(self, script_data: Dict, channel_type: str) -> Dict:
        """Fallback processing when LLM is not available."""
        scenes = script_data.get('scenes', [])
        enhanced_scenes = []
        
        for i, scene in enumerate(scenes):
            scene_text = scene if isinstance(scene, str) else scene.get('description', f'Scene {i+1}')
            enhanced_scenes.append(self._handle_llm_scene_failure(scene_text, i + 1, channel_type))
        
        processed_script = script_data.copy()
        processed_script['enhanced_scenes'] = enhanced_scenes
        processed_script['llm_processed'] = False
        
        return processed_script
    
    def generate_video(self, prompt: str, duration: float = 5.0, output_path: Optional[str] = None) -> Optional[str]:
        """Generate video using real AI models."""
        if not output_path:
            output_path = f"generated_video_{int(time.time())}.mp4"
        
        try:
            from ..text_to_video_generator import TextToVideoGenerator
            
            if not hasattr(self, 'video_generator') or self.video_generator is None:
                self.video_generator = TextToVideoGenerator(
                    vram_tier=self.vram_tier,
                    target_resolution=(1920, 1080)
                )
            
            scene_type = self._classify_scene_type(prompt)
            model_name = self.video_generator.get_best_model_for_content(scene_type, self.vram_tier)
            
            logger.info(f"Generating video with AI model {model_name}: {prompt[:50]}...")
            
            success = self.video_generator.generate_video(
                prompt=prompt,
                model_name=model_name,
                output_path=output_path,
                duration=duration,
                scene_type=scene_type
            )
            
            if success and os.path.exists(output_path):
                logger.info(f"AI video generated successfully: {output_path}")
                return output_path
            else:
                logger.error(f"AI video generation failed, creating fallback")
                return self._create_efficient_video(prompt, duration, output_path)
            
        except Exception as e:
            logger.error(f"Error in AI video generation: {e}")
            logger.error("AI video generation failed, attempting alternative models")
            return self._handle_video_generation_failure(prompt, duration, output_path)
    
    def _create_efficient_video(self, prompt: str, duration: float, output_path: str) -> str:
        """Create efficient video optimized for CPU systems."""
        try:
            import cv2
            import numpy as np
            import math
            
            if not output_path:
                output_path = f"efficient_video_{int(time.time())}.mp4"
            
            if isinstance(duration, (str, Path)):
                duration = 5.0  # Default duration
            duration = float(duration)
            
            fps = 24
            max_duration = min(duration, 10.0)
            frames = int(max_duration * fps)
            width, height = 1920, 1080  # 16:9 aspect ratio
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            base_frame = np.full((height, width, 3), [60, 80, 100], dtype=np.uint8)
            
            cv2.putText(base_frame, f"{self.channel_type.upper()} CONTENT", 
                       (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 3)
            cv2.putText(base_frame, prompt[:50], 
                       (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (200, 200, 200), 2)
            
            for i in range(frames):
                frame = base_frame.copy()
                
                t = i / fps
                circle_x = int(width * 0.5 + 200 * math.sin(t * 2))
                circle_y = int(height * 0.5 + 100 * math.cos(t * 3))
                cv2.circle(frame, (circle_x, circle_y), 50, (255, 200, 100), -1)
                
                cv2.putText(frame, f"Frame {i+1}/{frames}", 
                           (width - 300, height - 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
                
                out.write(frame)
            
            out.release()
            
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logger.info(f"CPU-optimized video created: {output_path} ({file_size} bytes)")
                return output_path
            else:
                logger.error("Failed to create efficient video")
                return self._handle_video_generation_failure(prompt, max_duration, output_path)
                
        except Exception as e:
            logger.error(f"Efficient video creation failed: {e}")
            safe_duration = float(duration) if not isinstance(duration, (str, Path)) else 5.0
            return self._handle_video_generation_failure(prompt, min(safe_duration, 10.0), output_path)
    
    def _parse_prompt_for_scenes(self, prompt: str) -> List[Dict]:
        """Parse prompt to extract scene information."""
        scenes = []
        
        if "combat" in prompt.lower() or "fight" in prompt.lower():
            scenes.append({"type": "combat", "intensity": "high", "colors": ["red", "orange", "yellow"]})
        if "peaceful" in prompt.lower() or "calm" in prompt.lower():
            scenes.append({"type": "peaceful", "intensity": "low", "colors": ["blue", "green", "white"]})
        if "dramatic" in prompt.lower() or "intense" in prompt.lower():
            scenes.append({"type": "dramatic", "intensity": "high", "colors": ["purple", "black", "white"]})
        
        if not scenes:
            scenes.append({"type": "general", "intensity": "medium", "colors": ["blue", "green", "yellow"]})
        
        for scene in scenes:
            scene["characters"] = self._extract_characters_from_prompt(prompt)
            scene["environment"] = self._extract_environment_from_prompt(prompt)
            scene["prompt"] = prompt
        
        return scenes
    
    def _extract_characters_from_prompt(self, prompt: str) -> List[str]:
        """Extract character information from prompt."""
        characters = []
        
        if "character" in prompt.lower():
            characters.append("main_character")
        if "hero" in prompt.lower() or "protagonist" in prompt.lower():
            characters.append("hero")
        if "villain" in prompt.lower() or "antagonist" in prompt.lower():
            characters.append("villain")
        
        return characters if characters else ["character"]
    
    def _extract_environment_from_prompt(self, prompt: str) -> str:
        """Extract environment information from prompt."""
        if "forest" in prompt.lower():
            return "forest"
        elif "city" in prompt.lower() or "urban" in prompt.lower():
            return "city"
        elif "space" in prompt.lower() or "cosmic" in prompt.lower():
            return "space"
        elif "ocean" in prompt.lower() or "water" in prompt.lower():
            return "ocean"
        else:
            return "generic"
    
    def _generate_scene_frame(self, scene_data: Dict, frame_idx: int, total_frames: int, width: int, height: int) -> np.ndarray:
        """Generate optimized frame for CPU efficiency."""
        import cv2
        import numpy as np
        import math
        
        base_color = 60 + (frame_idx % 40)
        frame = np.full((height, width, 3), [base_color, base_color + 20, base_color + 40], dtype=np.uint8)
        
        t = frame_idx / max(1, total_frames - 1)
        center_x = int(width * 0.5 + 100 * math.sin(t * 4))
        center_y = int(height * 0.5)
        cv2.circle(frame, (center_x, center_y), 30, (255, 255, 255), -1)
        
        scene_type = scene_data.get('type', 'default')
        cv2.putText(frame, f"Scene: {scene_type}", 
                   (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
        
        return frame
    
    def _generate_environment_background(self, frame: np.ndarray, environment: str, t: float, width: int, height: int) -> np.ndarray:
        """Generate simple environment background for CPU efficiency."""
        import numpy as np
        
        if environment == "forest":
            gradient = np.linspace(80, 40, height).reshape(-1, 1, 1)
            frame[:, :, 1] = gradient.squeeze()  # Green channel
                    
        elif environment == "city":
            gradient = np.linspace(60, 100, height).reshape(-1, 1, 1)
            frame[:, :] = gradient
                
        elif environment == "ocean":
            gradient = np.linspace(30, 80, height).reshape(-1, 1, 1)
            frame[:, :, 2] = gradient.squeeze()  # Blue channel
                
        else:
            gradient = np.linspace(60, 120, height).reshape(-1, 1, 1)
            frame[:, :] = gradient
        
        return frame
    
    def _add_characters_to_frame(self, frame: np.ndarray, characters: List[str], t: float, width: int, height: int) -> np.ndarray:
        """Add character representations to frame."""
        import cv2
        import math
        
        for i, character in enumerate(characters):
            char_x = int(width * (0.2 + 0.6 * i / max(1, len(characters) - 1)) + 50 * math.sin(t * 2 + i))
            char_y = int(height * 0.7 + 30 * math.cos(t * 3 + i))
            
            if character == "hero":
                cv2.circle(frame, (char_x, char_y), 40, (255, 255, 100), -1)
                cv2.ellipse(frame, (char_x - 20, char_y + 20), (30, 50), 0, 0, 180, (0, 100, 255), -1)
            elif character == "villain":
                points = np.array([[char_x, char_y - 40], [char_x - 30, char_y + 20], 
                                 [char_x + 30, char_y + 20]], np.int32)
                cv2.fillPoly(frame, [points], (50, 50, 150))
            else:
                cv2.rectangle(frame, (char_x - 25, char_y - 40), 
                            (char_x + 25, char_y + 20), (150, 200, 100), -1)
        
        return frame
    
    def _add_scene_effects(self, frame: np.ndarray, scene_data: Dict, t: float, width: int, height: int) -> np.ndarray:
        """Add scene-specific visual effects."""
        import cv2
        import numpy as np
        import math
        
        scene_type = scene_data["type"]
        intensity = scene_data["intensity"]
        
        if scene_type == "combat":
            if intensity == "high":
                flash_intensity = int(100 * math.sin(t * 10))
                if flash_intensity > 50:
                    overlay = np.full_like(frame, flash_intensity)
                    frame = cv2.addWeighted(frame, 0.8, overlay, 0.2, 0)
            
            for i in range(20):
                particle_x = int((i * 73 + t * 200) % width)
                particle_y = int((i * 127 + t * 150) % height)
                cv2.circle(frame, (particle_x, particle_y), 3, (255, 200, 0), -1)
                
        elif scene_type == "peaceful":
            for i in range(10):
                float_x = int(width * 0.1 + (width * 0.8) * ((i + t * 0.1) % 1))
                float_y = int(height * 0.2 + 100 * math.sin(t + i))
                cv2.circle(frame, (float_x, float_y), 5, (255, 255, 255), -1)
                
        elif scene_type == "dramatic":
            center_x, center_y = width // 2, height // 2
            for radius in range(50, 300, 50):
                alpha = 0.1 * math.sin(t * 2 + radius * 0.01)
                if alpha > 0:
                    cv2.circle(frame, (center_x, center_y), radius, (255, 255, 255), 2)
        
        return frame
    
    def _add_text_overlay(self, frame: np.ndarray, scene_data: Dict, frame_idx: int, total_frames: int):
        """Add informative text overlay to frame."""
        import cv2
        
        text_lines = [
            f"{self.channel_type.upper()} - {scene_data['type'].title()} Scene",
            f"Environment: {scene_data['environment'].title()}",
            f"Frame {frame_idx + 1}/{total_frames}"
        ]
        
        for i, text in enumerate(text_lines):
            y_pos = 50 + i * 40
            cv2.putText(frame, text, (50, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
    
    def _handle_video_generation_failure(self, prompt: str, duration: float, output_path: str) -> str:
        """Handle video generation failure by trying alternative models."""
        try:
            alternative_models = ["zeroscope", "stable_video_diffusion", "animatediff"]
            for model_name in alternative_models:
                try:
                    video_model = self._load_video_model(model_name)
                    if video_model and hasattr(video_model, 'generate_video'):
                        success = video_model.generate_video(
                            prompt=prompt,
                            duration=duration,
                            output_path=output_path
                        )
                        if success and os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
                            logger.info(f"Successfully generated video with alternative model: {model_name}")
                            return output_path
                except Exception as e:
                    logger.warning(f"Alternative model {model_name} failed: {e}")
                    continue
            
            logger.error("All video models failed, cannot generate video")
            return output_path
            
        except Exception as e:
            logger.error(f"Error in video generation failure handling: {e}")
            return output_path
    
    def generate_voice(self, text: str, character_voice: str = "default", output_path: str = "", language: str = "en") -> str:
        """Generate voice audio using real AI models."""
        try:
            from ..ai_voice_generator import AIVoiceGenerator
            
            if not output_path:
                output_path = f"generated_voice_{int(time.time())}.wav"
            
            if not hasattr(self, 'voice_generator') or self.voice_generator is None:
                self.voice_generator = AIVoiceGenerator(vram_tier=self.vram_tier)
            
            model_name = self.voice_generator.get_best_model_for_language(language)
            
            logger.info(f"Generating voice with AI model {model_name}: {text[:50]}...")
            
            success = self.voice_generator.generate_voice(
                text=text,
                model_name=model_name,
                output_path=output_path,
                language=language,
                character_voice=character_voice
            )
            
            if success and os.path.exists(output_path):
                logger.info(f"AI voice generated successfully: {output_path}")
                return output_path
            else:
                logger.error(f"AI voice generation failed, attempting alternative models")
                return self._handle_voice_generation_failure(text, output_path)
            
        except Exception as e:
            logger.error(f"Error in AI voice generation: {e}")
            return self._handle_voice_generation_failure(text, output_path)
    
    def _handle_voice_generation_failure(self, text: str, output_path: str) -> str:
        """Handle voice generation failure by trying alternative models."""
        try:
            alternative_models = ["bark", "xtts", "tortoise"]
            for model_name in alternative_models:
                try:
                    voice_model = self.load_voice_model(model_name)
                    if voice_model and voice_model.get("generate"):
                        success = voice_model["generate"](text, output_path=output_path)
                        if success and os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
                            logger.info(f"Successfully generated voice with alternative model: {model_name}")
                            return output_path
                except Exception as e:
                    logger.warning(f"Alternative voice model {model_name} failed: {e}")
                    continue
            
            logger.error("All voice models failed, cannot generate audio")
            return output_path
            
        except Exception as e:
            logger.error(f"Error in voice generation failure handling: {e}")
            return output_path
    
    def generate_background_music(self, scene_description: str, duration: float = 30.0, output_path: str = "", scene_type: str = "default") -> str:
        """Generate background music using real AI models."""
        try:
            from ..ai_music_generator import AIMusicGenerator
            
            if not output_path:
                output_path = f"generated_music_{int(time.time())}.wav"
            
            if not hasattr(self, 'music_generator') or self.music_generator is None:
                self.music_generator = AIMusicGenerator(vram_tier=self.vram_tier)
            
            model_name = self.music_generator.get_best_model_for_content(scene_type, duration)
            
            logger.info(f"Generating music with AI model {model_name}: {scene_description[:50]}...")
            
            success = self.music_generator.generate_music(
                description=scene_description,
                model_name=model_name,
                output_path=output_path,
                duration=duration,
                scene_type=scene_type
            )
            
            if success and os.path.exists(output_path):
                logger.info(f"AI music generated successfully: {output_path}")
                return output_path
            else:
                logger.error(f"AI music generation failed, creating fallback")
                return self._handle_music_generation_failure(scene_description, duration, output_path)
            
        except Exception as e:
            logger.error(f"Error in AI music generation: {e}")
            return self._handle_music_generation_failure(scene_description, duration, output_path)
    
    def _handle_music_generation_failure(self, description: str = None, duration: float = 30.0, output_path: str = "", prompt: str = None) -> str:
        """Handle music generation failure by trying alternative models."""
        try:
            alternative_models = ["musicgen_small", "musicgen_medium", "musicgen_large"]
            music_prompt = description or prompt or "background music"
            
            for model_name in alternative_models:
                try:
                    music_model = self.load_music_model(model_name)
                    if music_model and music_model.get("generate"):
                        success = music_model["generate"](music_prompt, duration=duration, output_path=output_path)
                        if success and os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
                            logger.info(f"Successfully generated music with alternative model: {model_name}")
                            return output_path
                except Exception as e:
                    logger.warning(f"Alternative music model {model_name} failed: {e}")
                    continue
            
            logger.error("All music models failed, cannot generate music")
            return output_path if output_path else ""
                
        except Exception as e:
            logger.error(f"Error in music generation failure handling: {e}")
            return output_path if output_path else ""
    
    def cleanup_models(self):
        """Clean up loaded models to free memory."""
        try:
            if hasattr(self, 'video_generator') and self.video_generator:
                self.video_generator.force_cleanup_all_models()
                self.video_generator = None
            
            if hasattr(self, 'voice_generator') and self.voice_generator:
                self.voice_generator.force_cleanup_all_models()
                self.voice_generator = None
            
            if hasattr(self, 'music_generator') and self.music_generator:
                self.music_generator.force_cleanup_all_models()
                self.music_generator = None
            
            for model_name, model in self.models.items():
                try:
                    if hasattr(model, 'to'):
                        model.to('cpu')
                    elif isinstance(model, dict):
                        for key, submodel in model.items():
                            if hasattr(submodel, 'to'):
                                submodel.to('cpu')
                    del model
                except Exception as e:
                    logger.warning(f"Error cleaning up model {model_name}: {e}")
            
            self.models.clear()
            
            import gc
            gc.collect()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            logger.info("All models cleaned up successfully")
            
        except Exception as e:
            logger.error(f"Error in model cleanup: {e}")
    
    def parse_input_script(self, input_path: str) -> Dict:
        """Parse input script from various formats."""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if input_path.endswith('.json'):
                return json.loads(content)
            elif input_path.endswith('.yaml') or input_path.endswith('.yml'):
                return yaml.safe_load(content)
            else:
                return {
                    "title": f"{self.channel_type.title()} Content",
                    "description": content,
                    "scenes": [{"description": content, "duration": 300}],
                    "characters": [{"name": "Character1"}],
                    "locations": [{"name": "Location1"}]
                }
                
        except Exception as e:
            logger.error(f"Failed to parse input script: {e}")
            return {
                "title": f"{self.channel_type.title()} Content",
                "description": "Default content",
                "scenes": [{"description": "Default scene", "duration": 300}],
                "characters": [{"name": "Character1"}],
                "locations": [{"name": "Location1"}]
            }
    


    def extract_highlights_from_video(self, video_path: str, num_highlights: int = 5) -> List[Dict]:
        """Extract highlight moments from main video using motion analysis."""
        try:
            import cv2
            import numpy as np
            
            if not os.path.exists(video_path):
                logger.warning(f"Video file not found: {video_path}")
                return self._handle_highlight_extraction_failure(num_highlights)
            
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"Could not open video: {video_path}")
                return self._handle_highlight_extraction_failure(num_highlights)
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            
            if duration < 15:
                logger.warning(f"Video too short for highlights: {duration}s")
                cap.release()
                return self._handle_highlight_extraction_failure(num_highlights)
            
            highlights = []
            segment_duration = 15
            motion_scores = []
            
            prev_frame = None
            frame_idx = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                if prev_frame is not None:
                    diff = cv2.absdiff(prev_frame, gray)
                    motion_score = np.mean(diff)
                    motion_scores.append((frame_idx / fps, motion_score))
                
                prev_frame = gray
                frame_idx += 1
            
            cap.release()
            
            if not motion_scores:
                return self._handle_highlight_extraction_failure(num_highlights)
            
            motion_scores.sort(key=lambda x: x[1], reverse=True)
            
            for i, (timestamp, score) in enumerate(motion_scores[:num_highlights]):
                start_time = max(0, timestamp - segment_duration / 2)
                end_time = min(duration, start_time + segment_duration)
                
                highlights.append({
                    "start_time": start_time,
                    "end_time": end_time,
                    "duration": segment_duration,
                    "excitement_score": score,
                    "title": f"Highlight {i + 1}"
                })
            
            return highlights
            
        except Exception as e:
            logger.error(f"Error extracting highlights: {e}")
            return self._handle_highlight_extraction_failure(num_highlights)
    
    def _handle_highlight_extraction_failure(self, num_highlights: int) -> List[Dict]:
        """Handle highlight extraction failure by using basic time-based segments."""
        try:
            logger.warning("Video analysis failed, using time-based highlight extraction")
            
            highlights = []
            segment_duration = 15
            spacing = 30  # 30 seconds between highlights
            
            for i in range(min(num_highlights, 3)):
                start_time = i * spacing + 10  # Start 10 seconds in to avoid intro
                highlights.append({
                    "start_time": start_time,
                    "end_time": start_time + segment_duration,
                    "duration": segment_duration,
                    "excitement_score": 60.0 + (i * 10),  # Vary scores slightly
                    "title": f"Key Moment {i + 1}",
                    "extraction_method": "time_based_fallback"
                })
            
            return highlights
            
        except Exception as e:
            logger.error(f"Error in highlight extraction failure handling: {e}")
            return []

    def _calculate_segment_excitement(self, video_path: str, start_time: float, end_time: float, cap, fps: float) -> float:
        """Calculate excitement score for a video segment using motion analysis."""
        try:
            import cv2
            import numpy as np
            
            start_frame = int(start_time * fps)
            end_frame = int(end_time * fps)
            
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            motion_scores = []
            prev_frame = None
            
            for frame_num in range(start_frame, min(end_frame, start_frame + 300)):
                ret, frame = cap.read()
                if not ret:
                    break
                
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                if prev_frame is not None:
                    diff = cv2.absdiff(gray, prev_frame)
                    motion_score = np.mean(diff)
                    motion_scores.append(motion_score)
                
                prev_frame = gray
            
            if motion_scores:
                avg_motion = np.mean(motion_scores)
                max_motion = np.max(motion_scores)
                excitement = (avg_motion * 0.7 + max_motion * 0.3) / 255.0
                return min(excitement, 1.0)
            
            return 0.0
            
        except Exception as e:
            logger.error(f"Error calculating excitement: {e}")
            return 0.0

    def _classify_segment_type(self, excitement_score: float) -> str:
        """Classify segment type based on excitement score."""
        if excitement_score > 0.7:
            return "high_action"
        elif excitement_score > 0.4:
            return "medium_action"
        else:
            return "low_action"

    def create_short_from_highlight(self, video_path: str, highlight: Dict, output_path: str, short_number: int) -> Optional[Dict]:
        """Create a short video from a highlight segment."""
        try:
            import subprocess
            
            if not os.path.exists(video_path):
                logger.error(f"Source video not found: {video_path}")
                return None
            
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            start_time = highlight["start_time"]
            duration = highlight["duration"]
            
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-ss', str(start_time),
                '-t', str(duration),
                '-vf', 'scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920',
                '-c:v', 'libx264',
                '-preset', 'veryslow',
                '-crf', '15',
                '-c:a', 'aac',
                output_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0 and os.path.exists(output_path):
                title = self._generate_short_title(highlight, short_number)
                
                return {
                    "path": output_path,
                    "title": title,
                    "duration": duration,
                    "excitement_score": highlight["excitement_score"]
                }
            else:
                logger.error(f"FFmpeg failed: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"Error creating short: {e}")
            return None

    def _generate_short_title(self, highlight: Dict, short_number: int) -> str:
        """Generate engaging title for short based on channel type and excitement."""
        excitement_score = highlight.get("excitement_score", 50.0)
        
        if excitement_score > 70:
            titles = [
                f"Epic {self.channel_type.title()} Moment #{short_number}!",
                f"Insane {self.channel_type.title()} Action #{short_number}",
                f"Mind-Blowing {self.channel_type.title()} #{short_number}",
                f"Incredible {self.channel_type.title()} Scene #{short_number}"
            ]
        elif excitement_score > 40:
            titles = [
                f"Great {self.channel_type.title()} Moment #{short_number}",
                f"Cool {self.channel_type.title()} Scene #{short_number}",
                f"Amazing {self.channel_type.title()} #{short_number}",
                f"Epic {self.channel_type.title()} Clip #{short_number}"
            ]
        else:
            titles = [
                f"{self.channel_type.title()} Highlight #{short_number}",
                f"{self.channel_type.title()} Moment #{short_number}",
                f"{self.channel_type.title()} Scene #{short_number}",
                f"{self.channel_type.title()} Clip #{short_number}"
            ]
        
        import random
        return random.choice(titles)
    
    def ensure_output_dir(self, output_path: str) -> Path:
        """Ensure output directory exists."""
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir
    
    def create_manifest(self, output_dir: Path, **kwargs) -> str:
        """Create manifest file with pipeline results."""
        import json
        import time
        
        manifest = {
            "channel_type": self.channel_type,
            "timestamp": time.time(),
            "device": self.device,
            "vram_tier": self.vram_tier,
            **kwargs
        }
        
        manifest_file = output_dir / "manifest.json"
        with open(manifest_file, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        return str(manifest_file)
    
    def generate_youtube_metadata(self, scenes: List[Dict], output_dir: Path, language: str = "en") -> Dict:
        """Generate YouTube metadata including title, description, and tags."""
        try:
            import json
            import random
            
            channel_type = getattr(self, 'channel_type', 'content')
            
            # Extract character names for better title generation
            character_names = []
            for scene in scenes:
                if isinstance(scene, dict) and 'characters' in scene:
                    for char in scene['characters']:
                        if isinstance(char, dict) and 'name' in char:
                            character_names.append(char['name'])
                        elif isinstance(char, str):
                            character_names.append(char)
            
            unique_chars = list(set(character_names))[:2]
            char_string = " and ".join(unique_chars) if unique_chars else "Epic Heroes"
            
            title_prompt = f"Write a short YouTube title for {channel_type} content featuring {char_string}. Maximum 50 characters. Example: 'Epic Battle Adventure' or 'Heroes Unite'. Title only:"
            
            llm_model = self.load_llm_model()
            if llm_model:
                title = llm_model["generate"](title_prompt, max_tokens=15)
                if not title or len(title.strip()) == 0:
                    title = self._handle_llm_generation_failure(title_prompt, 15)
                    if not title or len(title.strip()) == 0:
                        title = f"Epic {channel_type.title()} Adventure - Episode {random.randint(1, 100)}"
                else:
                    title = title.strip()
                    title = title.replace('"', '').replace("'", "").replace('-!', '').replace('"-', '')
                    if title.startswith('"') and title.endswith('"'):
                        title = title[1:-1]
                    if not title.endswith(('.', '!', '?')):
                        title = title + "!"
            else:
                title = f"Epic {channel_type.title()} Adventure - Episode {random.randint(1, 100)}"
            
            if not title or len(title.strip()) == 0:
                title = f"Amazing {channel_type.title()} Content - Episode {random.randint(1, 100)}"
            
            description_prompt = f"Write YouTube description for {channel_type} episode. Include plot summary. Keep under 100 words:"
            
            if llm_model:
                description = llm_model["generate"](description_prompt, max_tokens=50)
                if not description or len(description.strip()) == 0:
                    description = self._handle_llm_generation_failure(description_prompt, 50)
                    if not description or len(description.strip()) == 0:
                        description = f"An epic {channel_type} adventure featuring amazing characters and thrilling action across {len(scenes)} incredible scenes! Experience the world of anime and manga like never before!"
                else:
                    description = description.strip()
                    description = description.replace('Title: [Show spoiler]', '').replace('Example Tit...', '')
                    description = description.replace('Title:', '').strip()
                    if description.startswith('"') and description.endswith('"'):
                        description = description[1:-1]
                    if not description.endswith(('.', '!', '?')):
                        description = description + "."
            else:
                description = f"An epic {channel_type} adventure featuring amazing characters and thrilling action across {len(scenes)} incredible scenes! Experience the world of anime and manga like never before!"
            
            if not description or len(description.strip()) == 0:
                description = f"Amazing {channel_type} content with incredible storytelling and epic scenes!"
            
            next_episode_prompt = f"List 3 next episode ideas for {channel_type}. Format: 1. Idea one 2. Idea two 3. Idea three"
            
            if llm_model:
                next_suggestions = llm_model["generate"](next_episode_prompt, max_tokens=30)
                if not next_suggestions or len(next_suggestions.strip()) == 0:
                    next_suggestions = self._handle_llm_generation_failure(next_episode_prompt, 30)
                    if not next_suggestions or len(next_suggestions.strip()) == 0:
                        next_suggestions = "1. New character introduction and power awakening\n2. Tournament arc with intense battles\n3. Emotional backstory and character development"
                else:
                    next_suggestions = next_suggestions.strip()
            else:
                next_suggestions = "1. New character introduction and power awakening\n2. Tournament arc with intense battles\n3. Emotional backstory and character development"
            
            if not next_suggestions or len(next_suggestions.strip()) == 0:
                next_suggestions = "1. Epic character development\n2. New challenges and adventures\n3. Exciting plot twists"
            
            if not next_suggestions.strip().endswith(('.', '!', '?')):
                next_suggestions = next_suggestions.strip() + "."
            
            tags = [
                channel_type, "anime", "manga", "adventure", "action", 
                "storytelling", "characters", "epic", "series", "episode"
            ]
            
            metadata = {
                "title": title.strip(),
                "description": description.strip(),
                "tags": tags,
                "next_episode_suggestions": next_suggestions.strip(),
                "language": language,
                "scenes_count": len(scenes),
                "channel_type": channel_type
            }
            
            from pathlib import Path
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            title_file = output_dir / "title.txt"
            description_file = output_dir / "description.txt"
            tags_file = output_dir / "tags.txt"
            next_episode_file = output_dir / "next_episode.txt"
            
            with open(title_file, 'w', encoding='utf-8') as f:
                f.write(f"Title: {title.strip()}\n\nGenerated for {channel_type} episode with {len(scenes)} scenes")
            
            with open(description_file, 'w', encoding='utf-8') as f:
                f.write(f"Title: \"{title.strip()}\"\n\n{description.strip()}\n\n🔥 Don't miss the next episode!\n👍 Like and Subscribe for more {channel_type} content!")
            
            with open(tags_file, 'w', encoding='utf-8') as f:
                f.write(", ".join(tags))
            
            with open(next_episode_file, 'w', encoding='utf-8') as f:
                f.write(f"Next Episode Suggestions:\n\n{next_suggestions.strip()}")
            
            metadata_file = output_dir / f"youtube_metadata_{language}.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Generated YouTube metadata for {language}: {title}")
            return metadata
            
        except Exception as e:
            logger.error(f"Error generating YouTube metadata: {e}")
            return {
                "title": f"Epic {getattr(self, 'channel_type', 'content').title()} Adventure",
                "description": "Amazing content with incredible storytelling!",
                "tags": ["content", "adventure", "epic"],
                "next_episode_suggestions": "More amazing content coming soon!",
                "language": language,
                "scenes_count": len(scenes) if scenes else 0,
                "channel_type": getattr(self, 'channel_type', 'content')
            }

        return str(output_dir / "manifest.json")



    async def execute_async(self, project_data: Dict[str, Any]) -> str:
        """Execute pipeline asynchronously."""
        try:
            logger.info(f"Starting async execution for {self.channel_type} pipeline")
            
            input_path = project_data.get('input_path', '')
            script_data = project_data.get('script_data', {})
            output_path = project_data.get('output_path', '/tmp/pipeline_output')
            base_model = project_data.get('base_model', 'stable_diffusion_1_5')
            lora_models = project_data.get('lora_models', [])
            lora_paths = project_data.get('lora_paths', {})
            language = project_data.get('language', 'en')
            render_fps = project_data.get('render_fps', 24)
            output_fps = project_data.get('output_fps', 24)
            frame_interpolation_enabled = project_data.get('frame_interpolation_enabled', True)
            
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                self.run_with_script_data,
                input_path,
                script_data,
                output_path,
                base_model,
                lora_models,
                lora_paths,
                None,
                None,
                render_fps,
                output_fps,
                frame_interpolation_enabled,
                language
            )
            
            logger.info(f"Async execution completed for {self.channel_type} pipeline")
            return result
            
        except Exception as e:
            logger.error(f"Async execution failed for {self.channel_type} pipeline: {e}")
            import traceback
            traceback.print_exc()
            
            output_dir = self.ensure_output_dir(project_data.get('output_path', '/tmp/pipeline_output'))
            fallback_video = output_dir / "fallback_video.mp4"
            self._handle_video_generation_failure("Pipeline execution failed", 300.0, str(fallback_video))
            return str(output_dir)

    def run_with_script_data(self, input_path: str, script_data: Dict, output_path: str, 
                           base_model: str = "stable_diffusion_1_5", 
                           lora_models: Optional[List[str]] = None, 
                           lora_paths: Optional[Dict[str, str]] = None, 
                           db_run=None, db=None, render_fps: int = 24, 
                           output_fps: int = 24, frame_interpolation_enabled: bool = True, 
                           language: str = "en") -> str:
        """Run pipeline with pre-parsed script data to avoid re-parsing."""
        try:
            logger.info(f"Starting {self.channel_type} pipeline with script data")
            
            if script_data and script_data.get('scenes'):
                expanded_script = self.expand_script_if_needed(script_data)
                logger.info(f"Using provided script data with {len(script_data.get('scenes', []))} scenes")
            else:
                parsed_script = self.parse_input_script(input_path) if input_path else {}
                expanded_script = self.expand_script_if_needed(parsed_script)
                logger.info(f"Parsed script from {input_path}")
            
            self._save_llm_expansion_results(expanded_script, output_path)
            
            output_dir = self.ensure_output_dir(output_path)
            
            scene_videos = []
            scenes = expanded_script.get('scenes', self._get_default_scenes())
            characters = expanded_script.get('characters', [])
            locations = expanded_script.get('locations', [])
            
            logger.info(f"Processing {len(scenes)} scenes with {len(characters)} characters")
            
            for i, scene in enumerate(scenes):
                scene_description = scene if isinstance(scene, str) else scene.get('description', f'Scene {i+1}')
                
                if isinstance(scene, dict):
                    character = scene.get('character', '')
                    dialogue = scene.get('dialogue', '')
                    location = scene.get('location', '')
                    
                    if character and dialogue:
                        scene_description = f"Character {character} says: '{dialogue}' in location: {location}"
                
                enhanced_prompt = self._enhance_prompt_for_channel(scene_description)
                
                scene_output = output_dir / f"scene_{i+1}.mp4"
                video_path = self.generate_video(enhanced_prompt, output_path=str(scene_output), duration=5.0)
                
                if video_path and os.path.exists(video_path):
                    scene_videos.append(video_path)
                    logger.info(f"Generated scene {i+1}: {video_path}")
            
            final_video = output_dir / "final_video.mp4"
            if scene_videos:
                self._combine_scene_videos(scene_videos, str(final_video))
            else:
                self._handle_video_generation_failure("Default content", 300.0, str(final_video))
            
            self.create_manifest(output_dir, 
                               scenes_generated=len(scene_videos),
                               final_video=str(final_video),
                               language=language)
            
            logger.info(f"{self.channel_type} pipeline completed: {final_video}")
            return str(output_dir)
            
        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}")
            output_dir = self.ensure_output_dir(output_path)
            error_log = output_dir / "pipeline_error.txt"
            with open(error_log, 'w') as f:
                f.write(f"Pipeline execution failed: {e}\n")
                f.write(f"Error occurred during pipeline processing\n")
            logger.error(f"Pipeline failed, error logged to: {error_log}")
            return str(output_dir)

    def _save_llm_expansion_results(self, expanded_script: Dict, output_path: str):
        """Save LLM expansion results to output directory for debugging."""
        try:
            import json
            from pathlib import Path
            
            output_dir = Path(output_path)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            llm_output_file = output_dir / "llm_expansion.json"
            with open(llm_output_file, 'w', encoding='utf-8') as f:
                json.dump(expanded_script, f, indent=2, ensure_ascii=False)
            
            scenes_file = output_dir / "processed_scenes.json"
            scenes_data = {
                "total_scenes": len(expanded_script.get('scenes', [])),
                "characters": expanded_script.get('characters', []),
                "locations": expanded_script.get('locations', []),
                "scenes": expanded_script.get('scenes', [])
            }
            with open(scenes_file, 'w', encoding='utf-8') as f:
                json.dump(scenes_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"LLM expansion results saved to {llm_output_file}")
            
        except Exception as e:
            logger.warning(f"Failed to save LLM expansion results: {e}")
